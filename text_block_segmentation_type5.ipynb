{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install importlib-metadata==3.2.0\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip install PyPDF2\n",
        "!pip install PyPDF2\n",
        "!pip install pdfminer.six\n",
        "!pip install PyMuPDF\n",
        "!pip install tika"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taZBmwiUxTr_",
        "outputId": "eb82b653-cede-4ea8-dfdf-7f9211fc488c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting importlib-metadata==3.2.0\n",
            "  Downloading importlib_metadata-3.2.0-py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata==3.2.0) (3.15.0)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.1.0\n",
            "    Uninstalling importlib-metadata-6.1.0:\n",
            "      Successfully uninstalled importlib-metadata-6.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.3 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.2.0 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.2.0 which is incompatible.\n",
            "flask 2.2.3 requires importlib-metadata>=3.6.0; python_version < \"3.10\", but you have importlib-metadata 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed importlib-metadata-3.2.0\n",
            "2023-04-06 12:45:47.936477: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-06 12:45:48.904564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip\n",
            "\u001b[31m  ERROR: HTTP error 404 while getting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not install requirement https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip because of HTTP error 404 Client Error: Not Found for url: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip for URL https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz!pip\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.9/dist-packages (from pdfminer.six) (40.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20221105\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.21.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.21.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tika\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tika) (67.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from tika) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->tika) (2022.12.7)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32641 sha256=b99b50cc93d2bc1274e5cba6ff271d2c97c7cca834cf15d3aaf2cd1144561bb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/56/18/e752060632d32c39c9c4545e756dad281f8504dafcfac02b95\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "import json\n",
        "from tika import parser\n",
        "import fitz\n",
        "from operator import itemgetter\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "from pdfminer.high_level import extract_text\n",
        "from spacy.matcher import Matcher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPk816FdiUrK",
        "outputId": "11d38bf2-2846-4267-b7d7-ec45e31435fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/extracted_data'\n",
        "!rm -r '/content/text_resumes'\n",
        "!rm -r '/content/json_data'\n",
        "!rm -r '/content/headings'"
      ],
      "metadata": {
        "id": "41oNIADDiYC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fedac958-c0de-4318-d176-8b2ac447c7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/extracted_data': No such file or directory\n",
            "rm: cannot remove '/content/text_resumes': No such file or directory\n",
            "rm: cannot remove '/content/json_data': No such file or directory\n",
            "rm: cannot remove '/content/headings': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/text_resumes'\n",
        "!mkdir '/content/extracted_data'\n",
        "!mkdir '/content/json_data'\n",
        "!mkdir '/content/headings'"
      ],
      "metadata": {
        "id": "i47bpQ1NidRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResumeInformationExtractor:\n",
        "  def __init__(self, directory_path):\n",
        "    self.directory = directory_path\n",
        "    self.text_directory = '/content/text_resumes'\n",
        "    self.block_directory = '/content/extracted_data'\n",
        "    self.json_directory = '/content/json_data'\n",
        "    self.headings_directory = '/content/headings'\n",
        "    self.store_text()\n",
        "    print(\"DONE\")\n",
        "\n",
        "  def fonts(self, doc, granularity=False):\n",
        "    styles = {}\n",
        "    font_counts = {}\n",
        "    for page in doc:\n",
        "      blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "      for b in blocks:\n",
        "          if b['type'] == 0:\n",
        "              for l in b[\"lines\"]:\n",
        "                  for s in l[\"spans\"]:\n",
        "                      if granularity:\n",
        "                          identifier = \"{0}_{1}_{2}_{3}\".format(s['size'], s['flags'], s['font'], s['color'])\n",
        "                          styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],\n",
        "                                                'color': s['color']}\n",
        "                      else:\n",
        "                          identifier = \"{0}\".format(s['size'])\n",
        "                          styles[identifier] = {'size': s['size'], 'font': s['font']}\n",
        "\n",
        "                      font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage\n",
        "\n",
        "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "    if len(font_counts) < 1:\n",
        "        raise ValueError(\"Zero discriminating fonts found!\")\n",
        "\n",
        "    return font_counts, styles\n",
        "\n",
        "  def font_tags(self, font_counts, styles):\n",
        "    p_style = styles[font_counts[0][0]]\n",
        "    p_size = p_style['size']\n",
        "    font_sizes = []\n",
        "    for (font_size, count) in font_counts:\n",
        "        font_sizes.append(float(font_size))\n",
        "    font_sizes.sort(reverse=True)\n",
        "    idx = 0\n",
        "    size_tag = {}\n",
        "    for size in font_sizes:\n",
        "        idx += 1\n",
        "        if size == p_size:\n",
        "            idx = 0\n",
        "            size_tag[size] = '<p>'\n",
        "        if size > p_size:\n",
        "            size_tag[size] = '<h{0}>'.format(idx)\n",
        "        elif size < p_size:\n",
        "            size_tag[size] = '<s{0}>'.format(idx)\n",
        "\n",
        "    return size_tag\n",
        "\n",
        "\n",
        "  def headers_para(self, doc, size_tag):\n",
        "    header_para = []\n",
        "    first = True\n",
        "    previous_s = {}\n",
        "\n",
        "    for page in doc:\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        for b in blocks:\n",
        "            if b['type'] == 0:\n",
        "                block_string = \"\"\n",
        "                for l in b[\"lines\"]:\n",
        "                    for s in l[\"spans\"]:\n",
        "                        if s['text'].strip():\n",
        "                            if first:\n",
        "                                previous_s = s\n",
        "                                first = False\n",
        "                                block_string = size_tag[s['size']] + s['text']\n",
        "                            else:\n",
        "                                if s['size'] == previous_s['size']:\n",
        "                                    if block_string and all((c == \"|\") for c in block_string):\n",
        "                                        block_string = size_tag[s['size']] + s['text']\n",
        "                                    if block_string == \"\":\n",
        "                                        block_string = size_tag[s['size']] + s['text']\n",
        "                                    else:\n",
        "                                        block_string += \" \" + s['text']\n",
        "                                else:\n",
        "                                    header_para.append(block_string)\n",
        "                                    block_string = size_tag[s['size']] + s['text']\n",
        "                                previous_s = s\n",
        "                    block_string += \"|\"\n",
        "                header_para.append(block_string)\n",
        "    return header_para\n",
        "\n",
        "\n",
        "\n",
        "  def get_headings(self,elements):\n",
        "    # resume_headings = ['Personal Information', 'Education', 'Academic Projects', 'Academic Achievements and Awards', 'Technical Skills', 'Work Experience', 'Projects', 'Certifications', 'Publications', 'Skills', 'Achievements and Awards', 'Volunteering', 'Extracurricular Activities', 'Leadership', 'Hobbies', 'References', 'Achievements', 'Experience']\n",
        "    headings_ = []\n",
        "    for item in elements:\n",
        "        if item.startswith(('<h1>', '<h2>')):\n",
        "            headings_.append(item)\n",
        "    headings = []\n",
        "    for item in headings_:\n",
        "        if '>' in item and len(item.split('>')) > 1:\n",
        "            text = item.split('>')[1].split('|')[0].strip()\n",
        "            headings.append(text)\n",
        "    # filtered_words = [word for word in headings if word.upper() in resume_headings]\n",
        "    return headings\n",
        "\n",
        "\n",
        "  def extract_data(self, text , headings):\n",
        "    regex_patterns = {}\n",
        "    for heading in headings:\n",
        "        regex_patterns[heading] = re.compile(r'^' + heading, re.IGNORECASE)\n",
        "\n",
        "    text_blocks = {}\n",
        "    for heading in headings:\n",
        "        text_blocks[heading] = ''\n",
        "\n",
        "    current_heading = None\n",
        "    i = 0\n",
        "\n",
        "    for line in text.split('\\n'):\n",
        "        found_heading = False\n",
        "        for heading, regex_pattern in regex_patterns.items():\n",
        "            if regex_pattern.match(line):\n",
        "                current_heading = heading\n",
        "                found_heading = True\n",
        "                break\n",
        "        if found_heading:\n",
        "            continue\n",
        "\n",
        "        if current_heading is not None:\n",
        "            text_blocks[current_heading] += line + '\\n'\n",
        "    return text_blocks\n",
        "\n",
        "  def get_file_content(self, doc):\n",
        "    # parsed = parser.from_file(path)\n",
        "    # result = parsed['content']\n",
        "    # if not result:\n",
        "    #   return result\n",
        "    # else:\n",
        "    #   result = \"\\n\".join([line for line in result.split(\"\\n\") if line.strip() != \"\"])\n",
        "    header_para = []\n",
        "    first = True\n",
        "    previous_s = {}\n",
        "\n",
        "    for page in doc:\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        for b in blocks:\n",
        "            if b['type'] == 0:\n",
        "                block_string = \"\"\n",
        "                for l in b[\"lines\"]:\n",
        "                    for s in l[\"spans\"]:\n",
        "                        if s['text'].strip():\n",
        "                            if first:\n",
        "                                previous_s = s\n",
        "                                first = False\n",
        "                                block_string = s['text']\n",
        "                            else:\n",
        "                                if s['size'] == previous_s['size']:\n",
        "                                    if block_string and all((c == \"|\") for c in block_string):\n",
        "                                        block_string = s['text']\n",
        "                                    if block_string == \"\":\n",
        "                                        block_string = s['text']\n",
        "                                    else:\n",
        "                                        block_string += \" \" + s['text']\n",
        "                                else:\n",
        "                                    header_para.append(block_string)\n",
        "                                    block_string = s['text']\n",
        "                                previous_s = s\n",
        "                header_para.append(block_string)\n",
        "    result = '\\n'.join(header_para)\n",
        "    return result\n",
        "\n",
        "  def store_text(self):\n",
        "    print(self.directory)\n",
        "    files = sorted(os.listdir(self.directory))\n",
        "\n",
        "    for file in files:\n",
        "        path = self.directory + '/' + file\n",
        "        doc = fitz.open(path)\n",
        "        text_data = self.get_file_content(doc)\n",
        "        if not text_data:\n",
        "          continue\n",
        "        with open(self.text_directory + '/' + file.split('.')[0] + '.txt', 'w') as f:\n",
        "            f.write(text_data)\n",
        "\n",
        "        font_counts, styles = self.fonts(doc, granularity=False)\n",
        "        size_tag = self.font_tags(font_counts, styles)\n",
        "        elements = self.headers_para(doc, size_tag)\n",
        "        headings = self.get_headings(elements)\n",
        "        with open(self.headings_directory + '/' + file.split('.')[0] + '.txt', 'w') as f:\n",
        "          f.write(str(headings))\n",
        "\n",
        "        text_blocks = self.extract_data(text_data , headings)\n",
        "        with open(self.block_directory + '/' + file.split('.')[0] + '.txt', 'w') as f:\n",
        "            for heading, text_block in text_blocks.items():\n",
        "                if text_block:\n",
        "                    f.write(heading.upper() + '\\n')\n",
        "                    f.write(text_block + '-' * 50 + '\\n')\n",
        "\n",
        "        with open(self.json_directory + '/' + file.split('.')[0] + '.json', 'w') as f:\n",
        "            non_empty_text_blocks = {key: value.replace('\\n', ' ') for key, value in text_blocks.items() if value}\n",
        "            json.dump(non_empty_text_blocks, f, indent=2)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "pm_728WRiiVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESUME_DIRECTORY = '/content/type5'\n",
        "ResumeInformationExtractor(RESUME_DIRECTORY)\n",
        "!zip -r /content/json_data5.zip /content/json_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn9OyHQbjWey",
        "outputId": "e0686ef4-5a85-40ca-81a3-70c2e1d724d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/type5\n",
            "DONE\n",
            "  adding: content/json_data/ (stored 0%)\n",
            "  adding: content/json_data/pdf_resume_944.json (deflated 48%)\n"
          ]
        }
      ]
    }
  ]
}